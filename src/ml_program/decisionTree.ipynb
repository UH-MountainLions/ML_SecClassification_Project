{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of Frameworks and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Flow ID', ' Source IP', ' Source Port', ' Destination IP',\n",
       "       ' Destination Port', ' Protocol', ' Timestamp', ' Flow Duration',\n",
       "       ' Total Fwd Packets', ' Total Backward Packets',\n",
       "       'Total Length of Fwd Packets', ' Total Length of Bwd Packets',\n",
       "       ' Fwd Packet Length Max', ' Fwd Packet Length Min',\n",
       "       ' Fwd Packet Length Mean', ' Fwd Packet Length Std',\n",
       "       'Bwd Packet Length Max', ' Bwd Packet Length Min',\n",
       "       ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s',\n",
       "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
       "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
       "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
       "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags',\n",
       "       ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags',\n",
       "       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',\n",
       "       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',\n",
       "       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',\n",
       "       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',\n",
       "       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',\n",
       "       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',\n",
       "       ' Average Packet Size', ' Avg Fwd Segment Size',\n",
       "       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk',\n",
       "       ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk',\n",
       "       ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',\n",
       "       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',\n",
       "       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',\n",
       "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',\n",
       "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
       "       ' Idle Max', ' Idle Min', ' Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_path = 'C:/Users/lenovo/DataSet/CIC-IDS-2017/GeneratedLabelledFlows/TrafficLabelling/'\n",
    "st_file = 'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "st_file2 = 'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv'\n",
    "# st_file_merged = 'merged.csv'\n",
    "# st_file = 'Friday-WorkingHours-Morning.pcap_ISCX.csv'\n",
    "# st_file = 'Monday-WorkingHours.pcap_ISCX.csv'\n",
    "# st_file = 'Tuesday-WorkingHours.pcap_ISCX.csv'\n",
    "encoding = 'utf_8'\n",
    "df_data = pd.read_csv(os.path.join(st_path, st_file), encoding=encoding)\n",
    "df_data2 = pd.read_csv(os.path.join(st_path, st_file2), encoding=encoding)\n",
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_path_test_file = 'C:/Users/lenovo/DataSet/MachineLearningCSV/MachineLearningCVE/'\n",
    "st_file_test_file = 'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "encoding = 'utf_8'\n",
    "df_data_test_file= pd.read_csv(os.path.join(st_path_test_file, st_file_test_file), encoding=encoding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the data to see if they can be merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that are in the first CSV file but not in the second:  {' Source IP', 'Flow ID', ' Protocol', ' Timestamp', ' Destination IP', ' Source Port'}\n",
      "Columns that are in the second CSV file but not in the first:  set()\n",
      "The CSV files have different columns.\n"
     ]
    }
   ],
   "source": [
    "def compare_csv_headers(df_data, df_data_test_file):\n",
    "    \"\"\"\n",
    "    Compare the headers (column names) of two CSV files using pandas and return a boolean indicating if they match.\n",
    "    \"\"\"\n",
    "    data_cols = set(df_data.columns)\n",
    "    test_cols = set(df_data_test_file.columns)\n",
    "    \n",
    "    if data_cols == test_cols:\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Columns that are in the first CSV file but not in the second: \", data_cols - test_cols)\n",
    "        print(\"Columns that are in the second CSV file but not in the first: \", test_cols - data_cols)\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "if compare_csv_headers(df_data, df_data_test_file):\n",
    "    print(\"The CSV files have the same columns.\")\n",
    "else:\n",
    "    print(\"The CSV files have different columns.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been merged and saved to merged.csv.\n"
     ]
    }
   ],
   "source": [
    "def merge_csv_files(file1, file2):\n",
    "    \"\"\"\n",
    "    Merge the data from two CSV files using pandas and save the result to a new file in the same directory.\n",
    "    \"\"\"\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    \n",
    "    # Merge the DataFrames\n",
    "    merged_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Get the directory of the input files\n",
    "    directory = os.path.dirname(file1)\n",
    "    \n",
    "    # Create the output file path\n",
    "    output_file = os.path.join(directory, 'merged.csv')\n",
    "    \n",
    "    # Write the merged DataFrame to a CSV file\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Example usage\n",
    "file1 = 'C:/Users/lenovo/DataSet/CIC-IDS-2017/GeneratedLabelledFlows/TrafficLabelling/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "file2 = 'C:/Users/lenovo/DataSet/CIC-IDS-2017/GeneratedLabelledFlows/TrafficLabelling/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv'\n",
    "\n",
    "merge_csv_files(file1, file2)\n",
    "print(\"The data has been merged and saved to merged.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43538020"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_merged = pd.read_csv(os.path.join(st_path, st_file_merged), encoding=encoding)\n",
    "df_data_merged.size\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the types of attacks within the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_data_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m filtered_df \u001b[39m=\u001b[39m df_data_merged[df_data_merged[\u001b[39m'\u001b[39m\u001b[39m Label\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mBENIGN\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m label_column \u001b[39m=\u001b[39m filtered_df[\u001b[39m'\u001b[39m\u001b[39m Label\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m unique_labels \u001b[39m=\u001b[39m label_column\u001b[39m.\u001b[39munique()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_data_merged' is not defined"
     ]
    }
   ],
   "source": [
    "filtered_df = df_data_merged[df_data_merged[' Label'] != 'BENIGN']\n",
    "label_column = filtered_df[' Label']\n",
    "unique_labels = label_column.unique()\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45043030"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain why we are dropping these columns\n",
    "df_features = df_data.drop(['Flow ID',' Source IP', 'Flow ID', ' Timestamp', ' Destination IP', ' Source Port'], axis=1)  # Features\n",
    "df_target = df_data[' Label']  # Target variable\n",
    "\n",
    "df_features_test_file = df_data_test_file  # Features\n",
    "df_target_test_file = df_data_test_file[' Label']  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the columns in the dataframe to check if they are strings\n",
    "for st_col in df_features.columns:\n",
    "    if df_features[st_col].dtypes not in ['int64', 'float64']:\n",
    "        print(df_features[st_col].dtypes)\n",
    "        df_features[st_col] = le.fit_transform(df_features[st_col])\n",
    "\n",
    "for st_col in df_features_test_file.columns:\n",
    "    if df_features_test_file[st_col].dtypes not in ['int64', 'float64']:\n",
    "        print(df_features_test_file[st_col].dtypes)\n",
    "        df_features_test_file[st_col] = le.fit_transform(df_features_test_file[st_col])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for columns with infinite values\n",
    "lt_columns = df_features[df_features.columns[df_features.max() == np.inf]].columns\n",
    "lt_columns_test_file = df_features_test_file[df_features_test_file.columns[df_features_test_file.max() == np.inf]].columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow Bytes/s\n",
      "20700000000.0\n",
      " Flow Packets/s\n",
      "30000000.0\n",
      "Flow Bytes/s\n",
      "2070000000.0\n",
      " Flow Packets/s\n",
      "3000000.0\n"
     ]
    }
   ],
   "source": [
    "# modify infinite values (10 x max)\n",
    "for st_column_inf in lt_columns:\n",
    "    print(st_column_inf)\n",
    "    df_column_aux = df_features[st_column_inf]\n",
    "    # identify the max value\n",
    "    vl_max_aux = df_column_aux[df_column_aux < np.inf].max()\n",
    "    print(vl_max_aux)\n",
    "    # .loc is important to modify the value in the dataframe\n",
    "    df_features.loc[df_features[st_column_inf] == np.inf, st_column_inf] = 10*vl_max_aux\n",
    "    \n",
    "for st_column_inf2 in lt_columns_test_file:\n",
    "    print(st_column_inf2)\n",
    "    df_column_aux = df_features_test_file[st_column_inf2]\n",
    "    # identify the max value\n",
    "    vl_max_aux = df_column_aux[df_column_aux < np.inf].max()\n",
    "    print(vl_max_aux)\n",
    "    # .loc is important to modify the value in the dataframe\n",
    "    df_features_test_file.loc[df_features_test_file[st_column_inf2] == np.inf, st_column_inf2] = 10*vl_max_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns inf Index([], dtype='object')\n",
      "columns inf Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# check if there are still columns with infinite values\n",
    "\n",
    "lt_columns = df_features[df_features.columns[df_features.max() == np.inf]].columns\n",
    "print('columns inf', lt_columns)\n",
    "\n",
    "lt_columns_test_file = df_features_test_file[df_features_test_file.columns[df_features_test_file.max() == np.inf]].columns\n",
    "print('columns inf', lt_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Search for the columns with NaN values\n",
    "for st_column_nan in df_features.columns:\n",
    "    df_column_aux = df_features[df_features[st_column_nan].isna()].copy()\n",
    "    if len(df_column_aux) > 0:\n",
    "        print(df_column_aux.transpose())\n",
    "        print(df_target[df_features[st_column_nan].isna()].transpose())\n",
    "        print(st_column_nan)\n",
    "        print('The total amount of NaNs are', len(df_features[df_features[st_column_nan].isna()]))\n",
    "        print(df_features[st_column_nan].describe())\n",
    "# Drop the rows with NaN values\n",
    "df_features.dropna(inplace=True)\n",
    "df_target = df_target[df_target.index.isin(df_features.index)]\n",
    "\n",
    "\n",
    "for st_column_nan2 in df_features_test_file.columns:\n",
    "    df_column_aux = df_features_test_file[df_features_test_file[st_column_nan].isna()].copy()\n",
    "    if len(df_column_aux) > 0:\n",
    "        print(df_column_aux.transpose())\n",
    "        print(df_target[df_features_test_file[st_column_nan].isna()].transpose())\n",
    "        print(st_column_nan)\n",
    "        print('The total amount of NaNs are', len(df_features_test_file[df_features_test_file[st_column_nan].isna()]))\n",
    "        print(df_features_test_file[st_column_nan].describe())\n",
    "# Drop the rows with NaN values\n",
    "df_features_test_file.dropna(inplace=True)\n",
    "df_target_test_file = df_target_test_file[df_target_test_file.index.isin(df_features_test_file.index)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "mt_features_scaled = scaler.fit_transform(df_features)\n",
    "\n",
    "mt_features_scaled_test_file = scaler.fit_transform(df_features_test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.32786588 -0.51521605 -0.18640844 ... -0.47836899 -0.39107531\n",
      "  -1.14464915]\n",
      " [ 2.33743334 -0.51521269 -0.25124679 ... -0.47836899 -0.39107531\n",
      "  -1.14464915]\n",
      " [ 2.33748396 -0.5152145  -0.25124679 ... -0.47836899 -0.39107531\n",
      "  -1.14464915]\n",
      " ...\n",
      " [ 2.65741191 -0.51521377 -0.25124679 ... -0.47836899 -0.39107531\n",
      "  -1.14464915]\n",
      " [ 2.65477959 -0.51521462 -0.18640844 ... -0.47836899 -0.39107531\n",
      "  -1.14464915]\n",
      " [ 2.65493146 -0.51521399 -0.25124679 ... -0.47836899 -0.39107531\n",
      "  -1.14464915]]\n",
      "[[ 2.32786588 -0.51521605 -0.18640844 ... -0.47836899 -0.39107531\n",
      "  -1.14464915]\n",
      " [ 2.33743334 -0.51521269 -0.25124679 ... -0.47836899 -0.39107531\n",
      "  -1.14464915]\n",
      " [ 2.33748396 -0.5152145  -0.25124679 ... -0.47836899 -0.39107531\n",
      "  -1.14464915]\n",
      " ...\n",
      " [ 2.65741191 -0.51521377 -0.25124679 ... -0.47836899 -0.39107531\n",
      "  -1.14464915]\n",
      " [ 2.65477959 -0.51521462 -0.18640844 ... -0.47836899 -0.39107531\n",
      "  -1.14464915]\n",
      " [ 2.65493146 -0.51521399 -0.25124679 ... -0.47836899 -0.39107531\n",
      "  -1.14464915]]\n"
     ]
    }
   ],
   "source": [
    "print(mt_features_scaled)\n",
    "print(mt_features_scaled_test_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mt_features_scaled\n",
    "y_train = df_target\n",
    "X_test = mt_features_scaled_test_file\n",
    "y_test = df_target_test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40283     BENIGN\n",
      "26370       DDoS\n",
      "50636     BENIGN\n",
      "52512     BENIGN\n",
      "219471    BENIGN\n",
      "           ...  \n",
      "119882    BENIGN\n",
      "103697    BENIGN\n",
      "131935      DDoS\n",
      "146870    BENIGN\n",
      "121961      DDoS\n",
      "Name:  Label, Length: 180592, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(mt_features_scaled,\n",
    "#                                                     df_target,\n",
    "#                                                     test_size=0.2,\n",
    "#                                                     random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "### Decision Tree Classifier - This is the key part of the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf2 = RandomForestClassifier()\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting and evaluating the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      1.00      1.00     97714\n",
      "        DDoS       1.00      1.00      1.00    128027\n",
      "\n",
      "    accuracy                           1.00    225741\n",
      "   macro avg       1.00      1.00      1.00    225741\n",
      "weighted avg       1.00      1.00      1.00    225741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 79 features, but RandomForestClassifier is expecting 76 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred2 \u001b[39m=\u001b[39m clf2\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(classification_report(y_test, y_pred2))\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\DataSet\\ProjectML\\.venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:820\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    800\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[39m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[39m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m     proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(X)\n\u001b[0;32m    822\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    823\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39margmax(proba, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\DataSet\\ProjectML\\.venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:862\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    860\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    861\u001b[0m \u001b[39m# Check data\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X)\n\u001b[0;32m    864\u001b[0m \u001b[39m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    865\u001b[0m n_jobs, _, _ \u001b[39m=\u001b[39m _partition_estimators(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\DataSet\\ProjectML\\.venv\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:602\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 602\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49mDTYPE, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    603\u001b[0m \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc):\n\u001b[0;32m    604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\DataSet\\ProjectML\\.venv\\lib\\site-packages\\sklearn\\base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 588\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    590\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\DataSet\\ProjectML\\.venv\\lib\\site-packages\\sklearn\\base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    390\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    392\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 79 features, but RandomForestClassifier is expecting 76 features as input."
     ]
    }
   ],
   "source": [
    "y_pred2 = clf2.predict(X_test)\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 102443 and the array at index 1 has size 225741",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Confusion Matrix\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mt_results \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate((np\u001b[39m.\u001b[39;49mmatrix(y_pred2), np\u001b[39m.\u001b[39;49mmatrix(y_test)))\n\u001b[0;32m      3\u001b[0m df_results \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(mt_results, index\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mtranspose()\n\u001b[0;32m      4\u001b[0m df_results[\u001b[39m'\u001b[39m\u001b[39mequals\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_results[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m df_results[\u001b[39m'\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 102443 and the array at index 1 has size 225741"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "mt_results = np.concatenate((np.matrix(y_pred2), np.matrix(y_test)))\n",
    "df_results = pd.DataFrame(mt_results, index=['pred', 'test']).transpose()\n",
    "df_results['equals'] = df_results['test'] == df_results['pred']\n",
    "\n",
    "vl_equals = len(df_results[df_results['equals'] == True])\n",
    "vl_len_data = len(df_results)\n",
    "print('total', vl_equals, vl_len_data, vl_equals/vl_len_data)\n",
    "\n",
    "df_ddos = df_results[df_results['test'] != 'BENIGN'].copy()\n",
    "vl_equals = len(df_ddos[df_ddos['equals'] == True])\n",
    "vl_len_data = len(df_ddos)\n",
    "print('Attack', vl_equals, vl_len_data, vl_equals/vl_len_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 225741 225741 1.0\n",
      "Attack 128027 128027 1.0\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "mt_results = np.concatenate((np.matrix(y_pred), np.matrix(y_test)))\n",
    "df_results = pd.DataFrame(mt_results, index=['pred', 'test']).transpose()\n",
    "df_results['equals'] = df_results['test'] == df_results['pred']\n",
    "\n",
    "vl_equals = len(df_results[df_results['equals'] == True])\n",
    "vl_len_data = len(df_results)\n",
    "print('total', vl_equals, vl_len_data, vl_equals/vl_len_data)\n",
    "\n",
    "df_ddos = df_results[df_results['test'] != 'BENIGN'].copy()\n",
    "vl_equals = len(df_ddos[df_ddos['equals'] == True])\n",
    "vl_len_data = len(df_ddos)\n",
    "print('Attack', vl_equals, vl_len_data, vl_equals/vl_len_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225736</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225737</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225738</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225739</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225740</th>\n",
       "      <td>BENIGN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225741 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pred test\n",
       "0       BENIGN  NaN\n",
       "1       BENIGN  NaN\n",
       "2       BENIGN  NaN\n",
       "3       BENIGN  NaN\n",
       "4       BENIGN  NaN\n",
       "...        ...  ...\n",
       "225736  BENIGN  NaN\n",
       "225737  BENIGN  NaN\n",
       "225738  BENIGN  NaN\n",
       "225739  BENIGN  NaN\n",
       "225740  BENIGN  NaN\n",
       "\n",
       "[225741 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(y_pred, columns=['pred'])\n",
    "df_results = pd.concat((df_results, pd.DataFrame(y_test, columns=['test'])), axis=1)\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
